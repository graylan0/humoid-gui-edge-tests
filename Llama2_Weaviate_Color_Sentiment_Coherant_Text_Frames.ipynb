{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3oq4S8reUyqwzi8GHZ3h7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graylan0/humoid-gui-edge-tests/blob/main/Llama2_Weaviate_Color_Sentiment_Coherant_Text_Frames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yhNOv2nRAmVD",
        "outputId": "1001e396-5d93-4371-d104-a36610e453fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m622.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
            "Installing collected packages: typing-extensions, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.105.0 starlette-0.27.0 typing-extensions-4.9.0\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.9.0)\n",
            "Installing collected packages: h11, uvicorn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 uvicorn-0.24.0.post1\n",
            "Collecting aiosqlite\n",
            "  Downloading aiosqlite-0.19.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiosqlite\n",
            "Successfully installed aiosqlite-0.19.0\n",
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-3.25.3-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Collecting validators<1.0.0,>=0.21.2 (from weaviate-client)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
            "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (41.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2023.11.17)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\n",
            "Installing collected packages: validators, authlib, weaviate-client\n",
            "Successfully installed authlib-1.2.1 validators-0.22.0 weaviate-client-3.25.3\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m940.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.23.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54388 sha256=d3ca1c665c30ee94993bc9a82bb15218c47ae9b0e4fdd10d24dcdbbe0d5b374d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.33.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.7-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.33 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.9.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2023.11.17)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.7 pennylane-0.33.1 pennylane-lightning-0.33.1 rustworkx-0.13.2 semantic-version-2.10.0\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
            "Collecting asyncio\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asyncio\n",
            "Successfully installed asyncio-3.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.10/dist-packages (0.19.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi\n",
        "!pip install nest-asyncio\n",
        "!pip install uvicorn\n",
        "!pip install aiosqlite\n",
        "!pip install weaviate-client\n",
        "!pip install summa\n",
        "!pip install pennylane\n",
        "!pip install aiohttp\n",
        "!pip install asyncio\n",
        "!pip install aiosqlite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOmn8v8C3ec",
        "outputId": "2d784467-a9de-4d41-9a3b-ecc9129d8fa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-16 00:40:48--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.24, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1702946448&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjk0NjQ0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=vi3L6qMUdnARUC8pLaYf1evLLV2r5-1a6pL2sgtXRfaLHoCgquJSniGJYvJchvJ-0KbvV66uEnxh3Rdi9sPErGUzexWJ9xAqh-W8kQMKasqBm3XERd82rd2eiUZKmHZ3PDp2b42JarOP91NP2ZRfT%7Er-Z%7EMQ5mECE8k%7Ekeoa584zmU%7EAsdATXjASk5nPqWxcQRMy89THU-dM8GKEs3SDonDEreY9-6I7OWenzqSsWKrIUuLl1GQiS3LslXQdNM9IAUsuMzgU0M7cFOGT-LkU5qae%7E-3FnEX5-%7EHMKkePNdVBEPv-nrGhHrr5GZSDbTgs5%7E9mO3i8f9fwbWZ%7ER-0sJg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-16 00:40:48--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1702946448&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjk0NjQ0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=vi3L6qMUdnARUC8pLaYf1evLLV2r5-1a6pL2sgtXRfaLHoCgquJSniGJYvJchvJ-0KbvV66uEnxh3Rdi9sPErGUzexWJ9xAqh-W8kQMKasqBm3XERd82rd2eiUZKmHZ3PDp2b42JarOP91NP2ZRfT%7Er-Z%7EMQ5mECE8k%7Ekeoa584zmU%7EAsdATXjASk5nPqWxcQRMy89THU-dM8GKEs3SDonDEreY9-6I7OWenzqSsWKrIUuLl1GQiS3LslXQdNM9IAUsuMzgU0M7cFOGT-LkU5qae%7E-3FnEX5-%7EHMKkePNdVBEPv-nrGhHrr5GZSDbTgs5%7E9mO3i8f9fwbWZ%7ER-0sJg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.27, 18.154.185.64, 18.154.185.26, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   6.67G   162MB/s    in 63s     \n",
            "\n",
            "2023-12-16 00:41:51 (108 MB/s) - ‘llama-2-7b-chat.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZd28nTTAn6D",
        "outputId": "d13de4a7-84a1-4cc7-f8a8-8395943c34f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811155 sha256=1c354b67d594c3a3a9d2aca32d6dfe3c5e81d040fa0bd6677f6d0464e9526d8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import logging\n",
        "import aiosqlite\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "from summa import summarizer\n",
        "from llama_cpp import Llama\n",
        "import weaviate\n",
        "import re\n",
        "import aiohttp\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "dev = qml.device('default.qubit', wires=20)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def quantum_circuit(amplitudes, color_codes):\n",
        "    for i, amp in enumerate(amplitudes):\n",
        "        qml.RY(amp * np.pi, wires=i)\n",
        "    for i, code in enumerate(color_codes):\n",
        "        qml.RZ(code * np.pi, wires=i + 10)\n",
        "    for i in range(19):\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "    return qml.state()\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=3900,\n",
        ")\n",
        "\n",
        "executor = ThreadPoolExecutor(max_workers=3)\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url=\"https://magnetic-poodle-apparent.ngrok-free.app\",\n",
        ")\n",
        "\n",
        "async def retrieve_movie_frames_by_theme(theme):\n",
        "    query = {\n",
        "        \"query\": {\n",
        "            \"nearText\": {\n",
        "                \"concepts\": [theme],\n",
        "                \"certainty\": 0.7\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    result = await client.query.get(\"MovieFrame\", [\"frame_text\", \"summary\"]).with_near_text(query).do()\n",
        "    return result['data']['Get']['MovieFrame'] if 'data' in result and 'Get' in result['data'] and 'MovieFrame' in result['data']['Get'] else []\n",
        "\n",
        "async def initialize_db():\n",
        "    async with aiosqlite.connect(\"movie_frames.db\") as db:\n",
        "        await db.execute(\"CREATE TABLE IF NOT EXISTS POOLDATA (frame_num INTEGER PRIMARY KEY, frame_text TEXT, summary TEXT);\")\n",
        "        await db.commit()\n",
        "\n",
        "async def insert_into_db(frame_num, frame_text, summary):\n",
        "    async with aiosqlite.connect(\"movie_frames.db\") as db:\n",
        "        await db.execute(\"INSERT INTO POOLDATA (frame_num, frame_text, summary) VALUES (?, ?, ?)\", (frame_num, frame_text, summary))\n",
        "        await db.commit()\n",
        "\n",
        "async def generate_html_color_code(text):\n",
        "    attempts = 0\n",
        "    color_code = None\n",
        "\n",
        "    while not color_code and attempts < 10:  # Limit the number of attempts to avoid infinite loops\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            1. Based on the sentiment of the following text, suggest an HTML color code in hex format (e.g., #FF5733 for excitement, #3498DB for calmness). 2. Ensure you include the required color code\n",
        "            3. Text: '{text}'\n",
        "            4. Enter Suggested Color Code:\"\"\"\n",
        "\n",
        "            response = llm(prompt, max_tokens=10)\n",
        "            if response and response['choices']:\n",
        "                color_code_match = re.search(r'#[0-9A-Fa-f]{6}', response['choices'][0]['text'])\n",
        "                if color_code_match:\n",
        "                    color_code = color_code_match.group()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in generating color code: {e}\")\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    if not color_code:\n",
        "        # Log that a random color is being used\n",
        "        logger.warning(\"Failed to generate color code. Using a random color.\")\n",
        "        # Generate a random hex color\n",
        "        color_code = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
        "\n",
        "    return color_code\n",
        "\n",
        "\n",
        "\n",
        "async def vectorized_sentiment_analysis(text):\n",
        "    url = \"https://normally-unified-scorpion.ngrok-free.app/v1/modules/text2vec-contextionary/vectorize\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\"text\": text}\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.post(url, headers=headers, json=data) as response:\n",
        "            if response.status == 200:\n",
        "                return await response.json()\n",
        "            else:\n",
        "                # Handle error or return a default value\n",
        "                return None\n",
        "\n",
        "async def generate_and_summarize_frame(last_frame, frame_num, frames, theme, past_summaries):\n",
        "    sentiment_vector_tasks = [vectorized_sentiment_analysis(summary) for summary in past_summaries]\n",
        "    sentiment_vectors = await asyncio.gather(*sentiment_vector_tasks)\n",
        "\n",
        "    # Replace None with a default value, e.g., a zero vector\n",
        "    sentiment_vectors = [vec if vec is not None else {\"vector\": [0.0] * 300} for vec in sentiment_vectors]\n",
        "\n",
        "    # Extract the actual vectors from the results\n",
        "    sentiment_vectors = [vec[\"vector\"] for vec in sentiment_vectors]\n",
        "\n",
        "    # Asynchronously gather color codes\n",
        "    color_code_tasks = [generate_html_color_code(summary) for summary in past_summaries]\n",
        "    color_codes = await asyncio.gather(*color_code_tasks)\n",
        "    color_codes = [int(code.lstrip('#'), 16) for code in color_codes]  # Strip '#' and convert to integers\n",
        "\n",
        "    quantum_state = quantum_circuit([np.linalg.norm(vec) for vec in sentiment_vectors], color_codes)\n",
        "    quantum_influence = np.mean(quantum_state)\n",
        "    continuation_prompt = f\"{last_frame} Quantum influence: {quantum_influence}\"\n",
        "    continuation_response = llm(continuation_prompt, max_tokens=200)\n",
        "    new_frame = continuation_response['choices'][0]['text'] if continuation_response['choices'] else None\n",
        "    if new_frame:\n",
        "        summary = summarizer.summarize(new_frame, words=50)\n",
        "        await insert_into_db(frame_num, new_frame, summary)\n",
        "        return new_frame\n",
        "    return None\n",
        "\n",
        "\n",
        "async def start_movie(topic):\n",
        "    await initialize_db()\n",
        "    classes = client.schema.get()\n",
        "    class_names = [cls['class'] for cls in classes['classes']] if 'classes' in classes else []\n",
        "    if \"MovieFrame\" not in class_names:\n",
        "        client.schema.create_class({\"class\": \"MovieFrame\", \"properties\": [{\"name\": \"frame_text\", \"dataType\": [\"text\"], \"vectorizer\": \"text2vec-contextionary\"}, {\"name\": \"summary\", \"dataType\": [\"text\"], \"vectorizer\": \"text2vec-contextionary\"}], \"vectorIndexType\": \"hnsw\", \"vectorizer\": \"text2vec-contextionary\"})\n",
        "    initial_prompt_response = llm(f\"Write a book about {topic}\", max_tokens=700)\n",
        "    initial_prompt = initial_prompt_response['choices'][0]['text'] if initial_prompt_response['choices'] else \"\"\n",
        "    frames = {\"0\": initial_prompt}\n",
        "    last_frame = initial_prompt\n",
        "    for i in range(1, 15):\n",
        "        frame_num = i * 10\n",
        "        past_summaries = list(frames.values())[-10:]\n",
        "        last_frame = await generate_and_summarize_frame(last_frame, frame_num, frames, topic, past_summaries) or last_frame\n",
        "    sanitized_topic = ''.join(e for e in topic if e.isalnum())[:50]\n",
        "    with open(f\"{sanitized_topic}_movie_frames.json\", \"w\") as f:\n",
        "        json.dump(frames, f, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(start_movie(\"Language Models for Creative Work\"))\n",
        "    del llm\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhGAYKlaAp2B",
        "outputId": "cec2d935-8206-4884-8729-4509a76f3d81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    }
  ]
}